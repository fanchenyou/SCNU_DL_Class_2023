{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uU2jmkIrOtYX",
      "metadata": {
        "id": "uU2jmkIrOtYX"
      },
      "outputs": [],
      "source": [
        "# This example is modified from\n",
        "\n",
        "# https://huggingface.co/gpt2\n",
        "# https://huggingface.co/blog/how-to-generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AC1HAFXv8bjP",
      "metadata": {
        "id": "AC1HAFXv8bjP"
      },
      "outputs": [],
      "source": [
        "# In google colab, make sure you install transformers\n",
        "# uncomment the following line for first-time execution\n",
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78df2fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "14104dd1",
      "metadata": {
        "id": "14104dd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OYP8btfjOsDw",
      "metadata": {
        "id": "OYP8btfjOsDw"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Load Tokenizer and Model\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "YBdqEe__KrWT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBdqEe__KrWT",
        "outputId": "8cdec91c-a2a4-487b-d369-f6c4d615da9b"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m/home/fan6/project/playllm/gpt2/GPT_2_generate.ipynb 单元格 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bscnu/home/fan6/project/playllm/gpt2/GPT_2_generate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mgpt2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bscnu/home/fan6/project/playllm/gpt2/GPT_2_generate.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m TFGPT2LMHeadModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/software/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1825\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1820\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     )\n\u001b[1;32m   1824\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1825\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1826\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1827\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1828\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1829\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1830\u001b[0m     )\n\u001b[1;32m   1832\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1833\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer."
          ]
        }
      ],
      "source": [
        "# you need to download gpt2 by yourself through vpn, if you run this locally\n",
        "# in colab, this works well\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KUQlnEGmKXQK",
      "metadata": {
        "id": "KUQlnEGmKXQK"
      },
      "outputs": [],
      "source": [
        "text = \"Name a good tennis player.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ame3z8ed8oRW",
      "metadata": {
        "id": "Ame3z8ed8oRW"
      },
      "source": [
        "# Method 1: Use Transformers Pipeline to direct generate new sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hr5ZY3a_8JMR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr5ZY3a_8JMR",
        "outputId": "e20a5fd4-2060-4a97-dd2c-fd6ff3f5f3dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Replace me by any text you\\'d like. You\\'re going to look like the guy who beat the drums in the \\'40s?\" I tried to keep a small smile.\\n\\n\\nCrazy as I thought that, he said, \"I wish I could meet you tomorrow, just to tell you what I\\'m thinking.\"\"Hey, I just wanted to get home and see him. I just tried to play him the music while he was playing. He played like a lot of people, and'},\n",
              " {'generated_text': 'Replace me by any text you\\'d like. Please leave it with any comments for this post and use #freenode for all comments.\\n\\nThis is the #freenode \"rpc\". It is a lightweight, low end fork of the Linux RPC protocol that allows users to implement RPC commands without needing to write or debug a lot of code. This fork also makes it easier for people to interact with the rpc on a wide variety of devices to make use of it without breaking'},\n",
              " {'generated_text': \"Replace me by any text you'd like.\\n\\nYou can also try to play the game through SMS or a regular browser (I just have a new device). I can be extremely responsive when the voice input is interrupted. If it's during a game, try to call us again and ask for confirmation.\\n\\nAlso, you can try getting some kind of email (with your e-mail address, no matter what) or call me at 603-965-0021 for\"},\n",
              " {'generated_text': \"Replace me by any text you'd like.\"},\n",
              " {'generated_text': 'Replace me by any text you\\'d like. I am your best friend.\"\\n\\nShe took his hand in hers.\\n\\n\"I\\'m only one of his friends.\"\\n\\nBut before you can finish taking away what you\\'re telling me, you\\'ve come to the conclusion that she isn\\'t even an important figure. When the news was out about the manhunt for the missing woman, it made it obvious the young man she loved most was no longer alive. And it\\'s very clear'}]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set_seed(42)\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "generator(text, max_length=100, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U3VgjdOR8vVj",
      "metadata": {
        "id": "U3VgjdOR8vVj"
      },
      "source": [
        "Method 2: We let the model generate (forward) and tokenize back to words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t9DwM63FAAyF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9DwM63FAAyF",
        "outputId": "f3671a21-e6f1-4d1e-fe95-813d9c75d584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
            "array([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]],\n",
            "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# tokenize input prompt\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "print(encoded_input)\n",
        "\n",
        "\n",
        "# check https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py#L40\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"max_new_tokens\": 16,\n",
        "    \"num_return_sequences\":10,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ngl2neZ661Y_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngl2neZ661Y_",
        "outputId": "697f2d21-79a4-4b17-eae7-be64cb2ed96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.3168885707855225 3.1891257762908936\n",
            "R1: Replace me by any text you'd like.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>   ====   R2: Replace me by any text you'd like.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like. Like\" so each two lines--yes!!!!! The corrolla will become   ====   R2: Replace me by any text you'd like. Like\" so each two lines--yes!!!!! The corrolla will become\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like. Checksum : 3F666257B901 Dect asoneTOL   ====   R2: Replace me by any text you'd like. Checksum : 3F666257B901 Dect asoneTOL\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>   ====   R2: Replace me by any text you'd like.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like. If you have a student account and'd like to use something like that, just   ====   R2: Replace me by any text you'd like. If you have a student account and'd like to use something like that, just\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like.\n",
            "\n",
            "Cool. So hi cdability. Interesting.\n",
            "\n",
            "Wish you   ====   R2: Replace me by any text you'd like.\n",
            "\n",
            "Cool. So hi cdability. Interesting.\n",
            "\n",
            "Wish you\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like. Camus is our school.\"\n",
            "\n",
            "13\n",
            "\n",
            "\n",
            "In the middle of the   ====   R2: Replace me by any text you'd like. Camus is our school.\"\n",
            "\n",
            "13\n",
            "\n",
            "\n",
            "In the middle of the\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like.\n",
            "\n",
            "My love.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>   ====   R2: Replace me by any text you'd like.\n",
            "\n",
            "My love.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like.\n",
            "\n",
            "Drop me a reply below!\n",
            "\n",
            "NOTE: Reattach this author   ====   R2: Replace me by any text you'd like.\n",
            "\n",
            "Drop me a reply below!\n",
            "\n",
            "NOTE: Reattach this author\n",
            "\n",
            "\n",
            "\n",
            "R1: Replace me by any text you'd like. Do not enter any other characters!'\n",
            "\n",
            "Soyuzsey frowned and didn   ====   R2: Replace me by any text you'd like. Do not enter any other characters!'\n",
            "\n",
            "Soyuzsey frowned and didn\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# check KV cache option\n",
        "# https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py#L100\n",
        "\n",
        "t1 = time()\n",
        "set_seed(42)\n",
        "output_tensors_1 = model.generate(encoded_input['input_ids'], use_cache=True, **generation_kwargs)\n",
        "#print(output)\n",
        "t2 = time()\n",
        "set_seed(42)\n",
        "output_tensors_2 = model.generate(encoded_input['input_ids'], use_cache=False, **generation_kwargs)\n",
        "t3= time()\n",
        "\n",
        "print(t2-t1, t3-t2)\n",
        "\n",
        "responses_1 = tokenizer.batch_decode(output_tensors_1)\n",
        "responses_2 = tokenizer.batch_decode(output_tensors_2)\n",
        "\n",
        "# Compute sentiment score\n",
        "for response_1, response_2 in zip(responses_1, responses_2):\n",
        "  print('R1:', response_1, '  ====  ', 'R2:', response_2)\n",
        "  print('\\n\\n')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
