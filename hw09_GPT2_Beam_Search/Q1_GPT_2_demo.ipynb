{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This example is modified from\n",
        "\n",
        "# https://huggingface.co/gpt2\n",
        "# https://huggingface.co/blog/how-to-generate"
      ],
      "metadata": {
        "id": "uU2jmkIrOtYX"
      },
      "id": "uU2jmkIrOtYX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In google colab, make sure you install transformers\n",
        "# uncomment the following line for first-time execution\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "AC1HAFXv8bjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897a8fa7-4895-407d-e5d9-aae1896b0db8"
      },
      "id": "AC1HAFXv8bjP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14104dd1",
      "metadata": {
        "id": "14104dd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In colab, this automatically downloads gpt2 model from Hugginface.\n",
        "# If you run this locally, you need to download gpt2 by yourself through vpn, and change to your local directory path."
      ],
      "metadata": {
        "id": "xNeNuCPvvISb"
      },
      "id": "xNeNuCPvvISb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "YBdqEe__KrWT"
      },
      "id": "YBdqEe__KrWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Use Transformers Pipeline to direct generate new sentences"
      ],
      "metadata": {
        "id": "Ame3z8ed8oRW"
      },
      "id": "Ame3z8ed8oRW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This utilizes pipeline tool, which can search for num_return_sequences of sentences. It's the easiest way to prompt and get response."
      ],
      "metadata": {
        "id": "97G9duLvtlFc"
      },
      "id": "97G9duLvtlFc"
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = \"Name a good tennis player.\"\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "generator(query_text, max_length=100, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr5ZY3a_8JMR",
        "outputId": "13ea1ee6-c19a-4ac8-81ef-9a279a773682"
      },
      "id": "hr5ZY3a_8JMR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_model = pipeline(\"question-answering\", model='gpt2')\n",
        "\n",
        "question = \"Who is the best tennis player?\"\n",
        "context = \"I am a tennis fan. I think the person with most number of Grand Slams is the best player.\"\n",
        "qa_model(question = question, context = context)\n",
        "\n"
      ],
      "metadata": {
        "id": "f5z8DgCV0jUE"
      },
      "id": "f5z8DgCV0jUE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out different mode you can choose, such as translation, question-answering, so on.\n",
        "https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html"
      ],
      "metadata": {
        "id": "Diw_2FcOvios"
      },
      "id": "Diw_2FcOvios"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: We let the model generate (forward) and decode back to words"
      ],
      "metadata": {
        "id": "U3VgjdOR8vVj"
      },
      "id": "U3VgjdOR8vVj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real code of \"generate()\" function is https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L1351\n",
        "\n",
        "You can read the following examples first, then check the code."
      ],
      "metadata": {
        "id": "Ul9JpLkJJ7wi"
      },
      "id": "Ul9JpLkJJ7wi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Question Answering"
      ],
      "metadata": {
        "id": "KFlEqLKN6s6L"
      },
      "id": "KFlEqLKN6s6L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is more flexible. For teaching purpose, this code lets you understand each step."
      ],
      "metadata": {
        "id": "8D_OvMe13jgD"
      },
      "id": "8D_OvMe13jgD"
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize input prompt\n",
        "question_text = \"What is 1+2?\"\n",
        "encoded_input = tokenizer(question_text, return_tensors='tf')\n",
        "print(encoded_input)\n"
      ],
      "metadata": {
        "id": "t9DwM63FAAyF"
      },
      "id": "t9DwM63FAAyF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can define generation args as a dictionary, or pass them manually in generate() function\n",
        "# check https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py#L40\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"max_new_tokens\": 16,\n",
        "    \"num_return_sequences\":10,\n",
        "}"
      ],
      "metadata": {
        "id": "wKMuP3qh3yJN"
      },
      "id": "wKMuP3qh3yJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decode the question to answer\n",
        "\n",
        "model_1 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model_2 = GPT2LMHeadModel.from_pretrained(\"danyaljj/gpt2_question_answering_squad2\")\n",
        "\n",
        "input_ids = tokenizer.encode(\"You are good at math. Q: What is one plus two ? A:\", return_tensors=\"pt\")\n",
        "outputs = model_1.generate(input_ids)\n",
        "print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "outputs = model_2.generate(input_ids)\n",
        "print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "qRfIkbam38Ae"
      },
      "id": "qRfIkbam38Ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Text Generation"
      ],
      "metadata": {
        "id": "gdRAfUra6jD8"
      },
      "id": "gdRAfUra6jD8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, read the API and understand the arguments of \"generate()\"\n",
        "\n",
        "https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/text_generation#transformers.TFGenerationMixin.generate"
      ],
      "metadata": {
        "id": "l_4ZyN-F-hQc"
      },
      "id": "l_4ZyN-F-hQc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Important Argument\n",
        "#   max_new_tokens -- length of output\n",
        "#   num_return_sequences -- number of returned responses\n",
        "#   use_cache -- use KV cache to speed inference, see next section"
      ],
      "metadata": {
        "id": "idFu0jwo-w7T"
      },
      "id": "idFu0jwo-w7T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy decoding -- choose most probable next-word"
      ],
      "metadata": {
        "id": "Tcgur2scP_Mc"
      },
      "id": "Tcgur2scP_Mc"
    },
    {
      "cell_type": "code",
      "source": [
        "# decode the input to complete the text\n",
        "model_3 = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt')\n",
        "model_inputs = tokenizer('I enjoy playing', return_tensors='pt')\n",
        "\n",
        "# generate 40 new tokens\n",
        "greedy_output = model_3.generate(**model_inputs, max_new_tokens=2)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "print(model_inputs.keys())"
      ],
      "metadata": {
        "id": "HTxMikWH51Oh"
      },
      "id": "HTxMikWH51Oh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search to generate diverse sentences"
      ],
      "metadata": {
        "id": "oFOGdvmzLBN6"
      },
      "id": "oFOGdvmzLBN6"
    },
    {
      "cell_type": "code",
      "source": [
        "# use beam search to generate several sentence candidates\n",
        "# read https://huggingface.co/blog/how-to-generate for details\n",
        "\n",
        "# activate beam search and early_stopping\n",
        "beam_output = model_3.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=5,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "# print(beam_output)\n",
        "for ix, out in enumerate(beam_output):\n",
        "  print(ix, tokenizer.decode(out, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "PLUpuOsl70vh"
      },
      "id": "PLUpuOsl70vh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use beam search to generate several sentence candidates\n",
        "# without repeatitive n-gram\n",
        "model_inputs = tokenizer('I enjoy playing', return_tensors='pt')\n",
        "\n",
        "# activate beam search and early_stopping\n",
        "beam_output = model_3.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=10,\n",
        "    num_beams=100,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=5,\n",
        "    use_cache=True,\n",
        "    no_repeat_ngram_size=2     # dont not allow similar 2-gram appear twice\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "# print(beam_output)\n",
        "for ix, out in enumerate(beam_output):\n",
        "  print(ix, tokenizer.decode(out, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "LQ05pAbXLHl8"
      },
      "id": "LQ05pAbXLHl8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Tutorial: KV Cache"
      ],
      "metadata": {
        "id": "pSTfG-70B4Z9"
      },
      "id": "pSTfG-70B4Z9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "check KV cache option\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/generation/configuration_utils.py#L100\n",
        "\n",
        "check generate() code https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L1351"
      ],
      "metadata": {
        "id": "A0cL-e_uCOYf"
      },
      "id": "A0cL-e_uCOYf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, check the speed of using or not using KV Cache"
      ],
      "metadata": {
        "id": "DRD_obCTCHSW"
      },
      "id": "DRD_obCTCHSW"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "t1 = time()\n",
        "output_tensors_1 = model_3.generate(**model_inputs, use_cache=True, max_new_tokens=40, num_beams=5, num_return_sequences=5)\n",
        "t2 = time()\n",
        "output_tensors_2 = model_3.generate(**model_inputs, use_cache=False, max_new_tokens=40, num_beams=5, num_return_sequences=5)\n",
        "t3= time()\n",
        "\n",
        "print('Use KV Cache time:', np.round(t2-t1,2))\n",
        "print('NOT USE KV Cache time is much longer:', np.round(t3-t2,2))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ngl2neZ661Y_"
      },
      "id": "Ngl2neZ661Y_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The use_cache=True/False does not affect the output. The responses should be the same. Let's check."
      ],
      "metadata": {
        "id": "2U5XLtPuC0hm"
      },
      "id": "2U5XLtPuC0hm"
    },
    {
      "cell_type": "code",
      "source": [
        "responses_1 = tokenizer.batch_decode(output_tensors_1)\n",
        "responses_2 = tokenizer.batch_decode(output_tensors_2)\n",
        "\n",
        "# Check use_cache=True(R1) and False(R2) if they are the same\n",
        "for response_1, response_2 in zip(responses_1, responses_2):\n",
        "  print('R1:', response_1, '\\n')\n",
        "  print('R2:', response_2,'\\n\\n')\n"
      ],
      "metadata": {
        "id": "dOHPAGksCsob"
      },
      "id": "dOHPAGksCsob",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check code and see what's the inference behavior of generate\n",
        "Review lecture notes again."
      ],
      "metadata": {
        "id": "oC3QSwf_I1hu"
      },
      "id": "oC3QSwf_I1hu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read [tutorial](https://r4j4n.github.io/blogs/posts/kv/) and [tutorial](https://mett29.github.io/posts/kv-cache/), understand that KV cache is at Attention level.\n",
        "\n",
        "Turning cache on, each ATT layer will store previous attention all K variables as K_list and all V variables as V_list. Then each step appends newly computed K and V to the list.\n",
        "\n",
        "In real code, you can check GPT-2 attention [code](https://github.com/huggingface/transformers/blob/ae093eef016533a3670561fa9e26addb42d446d1/src/transformers/models/gpt2/modeling_gpt2.py#L901) and see this behavior."
      ],
      "metadata": {
        "id": "7sRCTqKw_jtV"
      },
      "id": "7sRCTqKw_jtV"
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_1)\n"
      ],
      "metadata": {
        "id": "FJua8ZQTI9Qv"
      },
      "id": "FJua8ZQTI9Qv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}