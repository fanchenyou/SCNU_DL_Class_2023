{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# In google colab, make sure you install transformers\n",
        "# uncomment the following line for first-time execution\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "AC1HAFXv8bjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d403a6c-0368-41ef-ba09-0e45c49b4085"
      },
      "id": "AC1HAFXv8bjP",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14104dd1",
      "metadata": {
        "id": "14104dd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM\n",
        "\n",
        "set_seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "YBdqEe__KrWT"
      },
      "id": "YBdqEe__KrWT",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW Code Question: Implement your own Beam Search with Priority Queue"
      ],
      "metadata": {
        "id": "95CZUjqihleh"
      },
      "id": "95CZUjqihleh"
    },
    {
      "cell_type": "code",
      "source": [
        "import queue\n",
        "\n",
        "# initialize a pq\n",
        "K = 30\n",
        "\n",
        "# we generate a sentence with 10 new words, \"I enjoy [word]*10\"\n",
        "max_len=10\n",
        "model_inputs = tokenizer('I enjoy', return_tensors='pt')\n",
        "\n",
        "\n",
        "# repeat the generation loop\n",
        "\n",
        "for i in range(max_len):\n",
        "\n",
        "  # use gpt to decode one word at a time, DO NOT MODIFY any argument\n",
        "  out = model.generate(**model_inputs, use_cache=True, max_new_tokens=1,\n",
        "                         num_beams=50, num_return_sequences=K, output_scores=True,\n",
        "                         return_dict_in_generate=True)\n",
        "  sentences = out.sequences\n",
        "  sent_scores = out.sequences_scores\n",
        "  responses = tokenizer.batch_decode(sentences)\n",
        "  print(i,responses)\n",
        "\n",
        "  # TODO: put each sentence into a PQ with the score in sent_scores\n",
        "  # https://docs.python.org/3/library/queue.html\n",
        "  pq = queue.PriorityQueue()\n",
        "  # pq.put((score,?,?))\n",
        "  # TODO: tokenize all sentences in selected top-K sentences as new model_inputs\n",
        "  # use pq.get() K times to get K sents of largest scores\n",
        "\n",
        "  # TODO: additional step to use 2-Gram to further prune similar sentences\n",
        "  # for example, for first round, we have K=30 sequences as return\n",
        "  # for second round, each has K=30 new sentences with one additional predicted word, we now have 900\n",
        "  # we use Bi-Gram counter to find similar sentences in 900, and retain a portion of it (e.g., 900*10%=90) for diversity, since memory is limited\n",
        "  # then we put these them into PQ, retrieve only K=30 most probably sentences (also diverse)\n",
        "  # One heuristic example:\n",
        "  # A=\"I enjoy walking and talking in city\"\n",
        "  # B=\"I enjoy walking and running in city\"\n",
        "  # they have four same bigrams out of 7 words, you can set a threshold T such as\n",
        "  # if #same-bi-gram > T * len(A), retain only one of them.\n",
        "  # Try T=0.3, 0.4, 0.5, ....,\n",
        "\n",
        "\n",
        "\n",
        "# Out of loop\n",
        "# TODO: retrieval the top-K final sentences and print them out\n",
        "# bonus would be given to more diverse and more likely sentences returned\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI7Dr7Nshk4v",
        "outputId": "76cbf459-efe8-4aaf-96fb-48159e13cf0b"
      },
      "id": "qI7Dr7Nshk4v",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "1 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "2 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "3 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "4 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "5 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "6 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "7 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "8 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n",
            "9 ['I enjoy the', 'I enjoy it', 'I enjoy this', 'I enjoy my', 'I enjoy that', 'I enjoy being', 'I enjoy playing', 'I enjoy a', 'I enjoy your', 'I enjoy reading', 'I enjoy to', 'I enjoy watching', 'I enjoy writing', 'I enjoy having', 'I enjoy doing', 'I enjoy working', 'I enjoy using', 'I enjoy you', 'I enjoy all', 'I enjoy making', 'I enjoy seeing', 'I enjoy hearing', 'I enjoy them', 'I enjoy what', 'I enjoy going', 'I enjoy these', 'I enjoy our', 'I enjoy and', 'I enjoy getting', 'I enjoy learning']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}